{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Coding Parallel Algorithms\n",
    "\n",
    "```\n",
    "date: \"Block 10\"\n",
    "author: \"Daniel Lawson\"\n",
    "email: dan.lawson@bristol.ac.uk\n",
    "output: html_document\n",
    "version: 1.0.2\n",
    "```\n",
    "\n",
    "Here we will put to practice the ideas described in the lecture, notably:\n",
    "\n",
    "* Vectorisation,\n",
    "* Native Accumulation,\n",
    "* The **Map** function,\n",
    "* The **Reduce** function, and its **accumulate** variant,\n",
    "* The **multiprocessing** environment for parallelisation.\n",
    "\n",
    "Until the final section, everything is only about preparing for parallelism. **Parallel code in python is non-trivial** because it needs to access additional cores that it may not have permission for. There are many different implementations that variously:\n",
    "\n",
    "* Don't work across operating systems,\n",
    "* Don't work in Jupyter but work in native python,\n",
    "* Work in python 2/3 but not vice versa,\n",
    "* Any combination of the above.\n",
    "\n",
    "As a rule, **you should not include parallel code inside a notebook** because it is not likely to be cross-platform. By nature of needing to paralellise, it is worth outsourcing important parallel code to a script and running this remotely.\n",
    "\n",
    "You will find that everything here **works on Notable via Blackboard**. I have also provided script files to complete the same parallelisation in Windows via native python. However the script solution we identify is adequate and is the recommended solution, so if your project group wants to work with parallel code and anyone runs Windows, **This Is The Way**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Vectorisation\n",
    "\n",
    "Vectorisation is straightforward in python (and R). You should always try to code with vectors/arrays, though For loops are sometimes necessary.\n",
    "\n",
    "In python:\n",
    "\n",
    "* For loops aren't intrinsically worse, but they encourage poor coding practice.\n",
    "\n",
    "In R: \n",
    "\n",
    "* For loops are very inefficient. For efficiency you may have to go out of your way to vectorise.\n",
    "\n",
    "In C/C++:\n",
    "\n",
    "* OpenMP allows for loops to be parallelised without any additional effort - just remember to avoid using the results of previous loops.\n",
    "* Modern updates (from C++-11) include many explicit vectorisations, allowing map/reduce vectorisations to be exploited directly.\n",
    "\n",
    "First, an array representation reminder in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "b =\n",
      "[[1 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "b = np.array([[1,1],[1,1]])\n",
    "print(\"a =\")\n",
    "print(a)\n",
    "print(\"b =\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b =\n",
      "[[2 3]\n",
      " [4 5]]\n",
      "a - b =\n",
      "[[0 1]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "# substraction and addition\n",
    "\n",
    "print(\"a + b =\")\n",
    "print(a + b)\n",
    "print(\"a - b =\")\n",
    "print(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * b =\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Element wise multiplication\n",
    "print(\"a * b =\")\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a@b =\n",
      "[[3 3]\n",
      " [7 7]]\n",
      "np.dot(a,b) =\n",
      "[[3 3]\n",
      " [7 7]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "print(\"a@b =\")\n",
    "print(a@b)\n",
    "print(\"np.dot(a,b) =\")\n",
    "print(np.dot(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.shape =\n",
      "(3,)\n",
      "d.shape\n",
      "(3, 1)\n"
     ]
    }
   ],
   "source": [
    "# numpy array with one row\n",
    "c =  np.array([1,2,3])\n",
    "print(\"c.shape =\")\n",
    "print(c.shape)\n",
    "# numpy array with three rows\n",
    "d = np.array([[1],[2],[3]])\n",
    "print(\"d.shape\")\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "first element in the array, e[0,0] =\n",
      "1\n",
      "first row of the array. e[0,:] =\n",
      "[1 2 3]\n",
      "second column of the array. e[:,1] =\n",
      "[2 5 8]\n"
     ]
    }
   ],
   "source": [
    "# Define an 3x3 2d array\n",
    "e = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(e)\n",
    "print(\"first element in the array, e[0,0] =\")\n",
    "print(e[0,0])\n",
    "print(\"first row of the array. e[0,:] =\")\n",
    "print(e[0,:])\n",
    "print(\"second column of the array. e[:,1] =\")\n",
    "print(e[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a simulation to compare between vectorised and non-vectorised code. This is just a simple matrix times vector computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some test data and simulate results\n",
    "N=100000# Number of rows\n",
    "K=100 # Number of columns\n",
    "X = np.random.randn(N,K)\n",
    "W = np.random.rand(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_times_vector_forloop(X,W):\n",
    "    N,K=X.shape\n",
    "    # Initialize theta\n",
    "    forloop = []\n",
    "    for i in range(N):\n",
    "        hypo_i = 0\n",
    "        for j in range(K):\n",
    "            hypo_i += W[j]*X[i,j]\n",
    "        forloop.append(hypo_i)\n",
    "    return(forloop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.01 s, sys: 17.5 ms, total: 4.03 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forloop=matrix_times_vector_forloop(X,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 ms, sys: 3.41 ms, total: 16.3 ms\n",
      "Wall time: 4.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# matrix format\n",
    "vect = X@W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for loop\n",
      "[-7.071205071734369, -0.8932577605479217, -6.821716607725058, 6.532161564877307]\n",
      "vectorised\n",
      "[-7.07120507 -0.89325776 -6.82171661  6.53216156]\n"
     ]
    }
   ],
   "source": [
    "##Â Check the answers\n",
    "print(\"for loop\")\n",
    "print(forloop[0:4])\n",
    "print(\"vectorised\")\n",
    "print(vect[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHALLENGE:  Make a plot of how the two approaches change in performance as a function of N (and/or K). What is the computational scaling?\n",
    "\n",
    "See https://towardsdatascience.com/vectorization-implementation-in-machine-learning-ca652920c55d for an example that is much more extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = np.random.randint(0, 100, size=1000000)\n",
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum_diff_with_accumulate(x):\n",
    "     x = np.asarray(x)\n",
    "     return np.add.accumulate(x)[-1]\n",
    "def cumsum_diff(x):\n",
    "     sum_px = x[0]\n",
    "     for px in x[1:]:\n",
    "         sum_px = sum_px + px\n",
    "     return sum_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 202 ms, sys: 1.84 ms, total: 204 ms\n",
      "Wall time: 203 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49484814"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cumsum_diff(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.37 ms, sys: 5.56 ms, total: 11.9 ms\n",
      "Wall time: 8.85 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49484814"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cumsum_diff_with_accumulate(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "Mapping python (and R) is straightfoward. \n",
    "\n",
    "Source document: http://chryswoods.com/parallel_python/map.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_distance(point1, point2):\n",
    "    \"\"\"\n",
    "    Function to calculate and return the distance between\n",
    "    two points\n",
    "    \"\"\"\n",
    "    \n",
    "    dx2 = (point1[0] - point2[0]) ** 2\n",
    "    dy2 = (point1[1] - point2[1]) ** 2\n",
    "    dz2 = (point1[2] - point2[2]) ** 2\n",
    "    return math.sqrt(dx2 + dy2 + dz2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x7ff0681e8250>\n"
     ]
    }
   ],
   "source": [
    "points1 = [(1.0,1.0,1.0), (2.0,2.0,2.0), (3.0,3.0,3.0)]\n",
    "points2 = [(4.0,4.0,4.0), (5.0,5.0,5.0), (6.0,6.0,6.0)]\n",
    "\n",
    "distances = map(calc_distance, points1, points2)\n",
    "\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has not done the calculation! Instead it returns an object (an iterator) that would evaluate this computation. But it does so lazily.  To get the answer, we must use the result, for example, by coercing it to a list.\n",
    "\n",
    "This behaviour is **standard** in parallel processing environments, in which the computation may be performed remotely and there may be additional remote computations to perform. By caching the computation, the software environment can sometimes obtain massive efficiency gains.\n",
    "\n",
    "This is how we get the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.196152422706632, 5.196152422706632, 5.196152422706632]\n"
     ]
    }
   ],
   "source": [
    "print(list(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, this time with multiple arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smallest(arg1, arg2, arg3):\n",
    "    \"\"\"\n",
    "    Function used to return the smallest value out \n",
    "    of 'arg1', 'arg2' and 'arg3'\n",
    "    \"\"\"\n",
    "\n",
    "    return min(arg1, min(arg2, arg3))\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [5, 4, 3, 2, 1]\n",
    "c = [1, 2, 1, 2, 1]\n",
    "\n",
    "result = map(find_smallest, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHALLENGE: Generalise calc_distance so that it can accept points in any numbers of dimensions.  Generalise find_smallest so that it can accept any number of arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Part 3: Reducing\n",
    "\n",
    "This is also straightfoward, and exists anagously in R.\n",
    "\n",
    "See http://chryswoods.com/parallel_python/reduce.html\n",
    "\n",
    "Lets start by defining a mapping problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 9, 11, 13, 15]\n"
     ]
    }
   ],
   "source": [
    "def add(x, y):\n",
    "    \"\"\"Function to return the sum of x and y\"\"\"\n",
    "    return x + y\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [6, 7, 8, 9, 10]\n",
    "\n",
    "result = map(add, a, b)\n",
    "\n",
    "print(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we use reduce to automatically apply addition to the entire set of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "?add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "result = map(add, a, b)\n",
    "\n",
    "total = reduce(add, result)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce has an optional third argument which is the initial value that is used as the first value for the reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "result = map(add, a, b)\n",
    "total = reduce(add, result, 10)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard \"reduce\" function does *not* do anything clever with the computation tree. It simply evaluates the reduction using the sequential definition. That means that it does *not* assume commutivity, which means it can be used with other operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat dog mouse fish\n"
     ]
    }
   ],
   "source": [
    "def join_strings(x, y):\n",
    "    return \"%s %s\" % (x,y)\n",
    "c = [\"cat\", \"dog\", \"mouse\", \"fish\"]\n",
    "\n",
    "result = reduce(join_strings, c)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accumulate vs reduce in python\n",
    "\n",
    "Python defines **reduce** to give only the final answer, whereas **accumulate** gives the running total as a list (via an iterator, like map).\n",
    "\n",
    "This is not a universally recognised separation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 16, 27, 40, 55]\n"
     ]
    }
   ],
   "source": [
    "from itertools import accumulate\n",
    "\n",
    "result = map(add, a, b)\n",
    "total = accumulate(result, add)\n",
    "print(list(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cat dog', 'cat dog mouse', 'cat dog mouse fish']\n"
     ]
    }
   ],
   "source": [
    "print(list(accumulate(c, join_strings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: parallel implementation\n",
    "\n",
    "multiprocessing python code has to be written into a text file and executed using the python interpreter. It is not recommended to try to run a multiprocessing python script interactively, e.g. via ipython or ipython notebook.\n",
    "\n",
    "This is because the required resources (CPUs) have to be requested from the system and appropriately returned, and the libraries are not reliable across platforms.\n",
    "\n",
    "(it seems to work on linux and mac, but not in windows https://stackoverflow.com/questions/37103243/multiprocessing-pool-in-jupyter-notebook-works-on-linux-but-not-windows)\n",
    "\n",
    "See http://chryswoods.com/parallel_python/multiprocessing.html\n",
    "\n",
    "Multiprocessing achieves parallelism by running multiple copies of your script, it forces you to write it in a particular way. All imports should be at the top of the script, followed by all function and class definitions. This is to ensure that all copies of the script have access to the same modules, functions and classes. Then, you should ensure that only the master copy of the script runs the code by protecting it behind an ``if __name__ == \"__main__\"`` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function for multiprocessing functionality within Jupyter:\n",
    "\n",
    "If the following works you should be ok in the notebook; otherwise switch to the scripts, which are run from within jupyter below.\n",
    "\n",
    "**HEALTH WARNING: The following code willl CRASH your python kernel in Windows and MacOS Big Sur, which will need RESTARTING!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## this is a test function, reporting happens further down.\n",
    "from multiprocessing import Pool\n",
    "def f(x):\n",
    "    return x**2\n",
    "if __name__ == \"__main__\":\n",
    "    pool = Pool(4)\n",
    "    pool.map(f,range(10))[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â A Fully functional multiprocessing map and reduce example\n",
    "\n",
    "The following script illustrates the key points:\n",
    "\n",
    "* Distributing compute over cores\n",
    "* Detecting the CPU architecture/count\n",
    "* Ensuring the compute is parallelised\n",
    "* Performing process-specific evaluation\n",
    "\n",
    "Remember, don't run this if parallel in Jupyter isn't working on your machine. Skip to the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from multiprocessing import Pool, cpu_count, current_process\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Function to return the square of the argument\"\"\"\n",
    "    print(\"Worker %s calculating square of %s\" % (current_process().pid, x))\n",
    "    return x * x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print the number of cores\n",
    "    print(\"Number of cores available equals %s\" % cpu_count())\n",
    "    N=50\n",
    "    # create a pool of workers\n",
    "    # start all worker processes\n",
    "    pool = Pool(processes= cpu_count())\n",
    "    def myfunction(x):\n",
    "        print(x)\n",
    "    # create an array of 5000 integers, from 1 to 5000\n",
    "    r = range(1, N+1)\n",
    "    ## NB: TRY CHANGING square to myfunction?\n",
    "    result = pool.map(square, r)\n",
    "\n",
    "    total = reduce(lambda x, y: x + y, result)\n",
    "\n",
    "    print(\"The sum of the square of the first %s integers is %s\" % (N, total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a parallel script from Jupyter\n",
    "\n",
    "The following code runs the python script `block10-parallel01.py` and **keeps the results in memory of this notebook**.\n",
    "\n",
    "First we take a look at the code we are about to run. Remember that Jupyter provides a pleasant environment for writing scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat block10-parallel01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we  run it **interactively**, meaning that all the memory that is accessible in the notebook is accessible in your script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores available equals 8\n",
      "The sum of the square of the first 50 integers is 42925\n",
      "0:00:00.129839\n"
     ]
    }
   ],
   "source": [
    "%run -i block10-parallel01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cores available equals 8\n",
      "The sum of the square of the first 50 integers is 42925\n",
      "0:00:00.129861\n"
     ]
    }
   ],
   "source": [
    "%run -i block10-parallel01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of the square of the first 50 integers is 42925\n"
     ]
    }
   ],
   "source": [
    "print(\"The sum of the square of the first %s integers is %s\" % (N, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the order of computation and printing?\n",
    "\n",
    "In general it is a really bad idea to assume that printing appears to the screen in the correct order!\n",
    "\n",
    "(NB: Christopher's code used \"with Pool(processes=nprocs) as pool\" which didn't work for me due to python version issues. Multicore processing is still in active development....)\n",
    "\n",
    "Note that you can change the code above to use a function defined inside the __main__ loop. This **hangs**  because the worker nodes can't see it!  **CAREFUL** with this sort of thing.\n",
    "\n",
    "However, we can reuse our pool of processes in a straightforward way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from multiprocessing import Pool, cpu_count, current_process\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Function to return the square of the argument\"\"\"\n",
    "    print(\"Worker %s calculating square of %s\" % (current_process().pid, x))\n",
    "    return x * x\n",
    "\n",
    "def cube(x):\n",
    "    \"\"\"Function to return the cube of the argument\"\"\"\n",
    "    print(\"Worker %s calculating cube of %s\" % (current_process().pid, x))\n",
    "    return x * x * x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print the number of cores\n",
    "    print(\"Number of cores available equals %s\" % cpu_count())\n",
    "    N=5\n",
    "    # create a pool of workers\n",
    "    # start all worker processes\n",
    "    pool = Pool(processes= cpu_count()) ## THIS is where all of the memory state is \n",
    "    ## created and all of the processes \"know about\" everything above. So they \"know\" N\n",
    "    ## and hence all compute their own version of r correctly.\n",
    "    \n",
    "    # create an array of 5000 integers, from 1 to N \n",
    "    r = range(1, N+1)\n",
    "    squares = pool.map(square, r)\n",
    "    print(\"The sum of the square of the first %s integers is %s\" % (N, totalsquares))\n",
    "    cubes = pool.map(cube, r)\n",
    "    print(\"The sum of the cube of the first %s integers is %s\" % (N, totalcubes))\n",
    "    totalsquares = reduce(lambda x, y: x + y, squares)\n",
    "    totalcubes = reduce(lambda x, y: x + y, cubes)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use mapping on multiple inputs, we have to either:\n",
    "\n",
    "* create a tuple of the arguments\n",
    "* or pass it through using **zip** and switch from the **map** to **starmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def add(x, y):\n",
    "    \"\"\"Return the sum of the tuple of two arguments\"\"\"\n",
    "    return x + y\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "b = [6, 7, 8, 9, 10]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool() as pool:\n",
    "        result = pool.starmap(add, zip(a,b))\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other implementation niggles, such as support for lambda functions (which is missing) etc. These may be addressed in newer versions or alternative packages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: asynchronous functions\n",
    "\n",
    "We saw above that the \"print\" statements were out of order. This is because the threads had to \"race\" to collect the next job, and also race to print to the screen. There is only one job queue and one screen.\n",
    "\n",
    "The following script performs jobs asynchronously. You should see that there are 3 workers, which complete one task before taking the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool, current_process\n",
    "\n",
    "def slow_function(nsecs):\n",
    "    \"\"\"\n",
    "    Function that sleeps for 'nsecs' seconds, returning\n",
    "    the number of seconds that it slept\n",
    "    \"\"\"\n",
    "    print(\"Process %s going to sleep for %s second(s)\" % (current_process().pid, nsecs))\n",
    "    # use the time.sleep function to sleep for nsecs seconds\n",
    "    time.sleep(nsecs)\n",
    "    print(\"Process %s waking up\" % current_process().pid)\n",
    "    return nsecs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Master process is PID %s\" % current_process().pid)\n",
    "\n",
    "    with Pool(3) as pool:\n",
    "        r = pool.map(slow_function, [8,2,3,4,5,6,7])\n",
    "\n",
    "    print(\"Result is %s\" % r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat block10-parallel02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i block10-parallel02.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we don't want to wait for a computation to be completed before distributing some other computation. \n",
    "\n",
    "The async versions of map, apply, etc return immediately, providing a \"future\" object. These are evaluated asyncyronously. They can be blocked (here via \"wait\") and allow a [callback](https://docs.python.org/3.8/library/multiprocessing.html?highlight=map_async#multiprocessing.pool.Pool.map_async) which runs on completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool, current_process\n",
    "\n",
    "def slow_function(nsecs):\n",
    "    \"\"\"\n",
    "    Function that sleeps for 'nsecs' seconds, returning\n",
    "    the number of seconds that it slept\n",
    "    \"\"\"\n",
    "    print(\"Process %s going to sleep for %s second(s)\" % (current_process().pid, nsecs))\n",
    "    # use the time.sleep function to sleep for nsecs seconds\n",
    "    time.sleep(nsecs)\n",
    "    print(\"Process %s waking up\" % current_process().pid)\n",
    "    return nsecs\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Master process is PID %s\" % current_process().pid)\n",
    "    r=[]\n",
    "    with Pool(3) as pool:\n",
    "        print(\"Starting r1 pool\")\n",
    "        r1 = pool.map_async(slow_function, [1,2,3,4,5],callback=r.extend)\n",
    "        # r1 starts first, finishes last\n",
    "        \n",
    "        ##Â Compare the following:\n",
    "        # r1.wait() # Uncomment this\n",
    "        print(\"Starting r2 pool\")\n",
    "        r2 = pool.map_async(slow_function, [0.5,0.5,0.5,0.5,0.5],callback=r.extend)\n",
    "\n",
    "        ## With this:\n",
    "        r1.wait() # Comment out this \n",
    "        \n",
    "        r2.wait()\n",
    "        print(\"Result one is %s\" % r1.get())\n",
    "        print(\"Result two is %s\" % r2.get())\n",
    "        print(\"Result appended is %s\" % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous computation \n",
    "\n",
    "Here we encountered \"futures\" which are computations that may not have completed and therefore may not be available.\n",
    "\n",
    "Futures are a very common variable type in parallel programming across many languages. Futures provide several common functions;\n",
    "\n",
    "* Block (wait) until the result is available. In multiprocessing, this is via the .wait() function, e.g. r1.wait() in the above script.\n",
    "* Retrieve the result when it is available (blocking until it is available). This is the .get() function, e.g. r1.get().\n",
    "* Test whether or not the result is available. This is the .ready() function, which returns True when the asynchronous function has finished and the result is available via .get().\n",
    "* Test whether or not the function was a success, e.g. whether or not an exception was raised when running the function. This is the .successful() function, which returns True if the asynchronous function completed without raising an exception. Note that this function should only be called after the result is available (e.g. when .ready() returns True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Information\n",
    "\n",
    "There are additional ways to help parallelisation be efficient. One is the idea of \"chunksize\", or how many commands get sent to each worker; it is a parameter of starmap/map."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
