{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop on Topic Modelling (Part 1, Prereqs)\n",
    "\n",
    "```\n",
    "date: \"Block 07\"\n",
    "author: \"Daniel Lawson\"\n",
    "email: dan.lawson@bristol.ac.uk\n",
    "output: html_document\n",
    "version: 1.0.1\n",
    "```\n",
    "\n",
    "## NLP Environment\n",
    "\n",
    "Here we set up the NLP environment in python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The libraries that are needed\n",
    "\n",
    "The main library for this is called \"gensim\". However, there are several libraries that implement better natural language processing technologies.  The main facility is something called \"stemming\" or \"lemmatizing\". \n",
    "\n",
    "https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "\n",
    "* **Stemming**: Cutting off the start and end of words to make (hopefully) consistent word stems\n",
    "* **Lemmatization**: Looking up the correct word-roots in dictionaries to find the morphological root of words\n",
    "\n",
    "Stemming may work across languages and dialects, and is computationally cheaper. However, it may merge words that are functionally different; for example, *operation*, *operational*, *operand* and *opera* might all become *oper*!\n",
    "\n",
    "Lemmatization works only as well as its dictionary and requires large databases to function. It would be preferred for many language tasks.\n",
    "\n",
    "An important additional step is to remove **stop words**. These are common words that link other words but have little intrinsic meaning by themselves, such as \"the\", \"it\", \"on\", \"and\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of software\n",
    "\n",
    "Here we install the required packages, nltk and gensim. Note that this is not completing the setup for these as they contain sub-modules that need configuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install gensim\n",
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check our gensim version.\n",
    "\n",
    "You can ignore this; but, you need gensim v3, not v0, for some of the functions to work. \n",
    "\n",
    "The command-line update is \"conda install -c anaconda gensim\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test that we can load all software we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (gensim.__version__)\n",
    "## If you need to reinstall and reload:\n",
    "##from importlib import reload\n",
    "##reload(gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to download the nltk modules that are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do this once! Then leave commented next time you run the script.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('popular') # Get the \"popular\" package with the advanced WordNet dictionary in it. There are ways to avoid this is you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, load the data. This idea comes from [Susan Li on Towards Data Science](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) and the data is direct from [Kaggle million headlines](https://www.kaggle.com/therohk/million-headlines/data). Because the documents are short, this is a useful dataset for teaching.\n",
    "\n",
    "We will download it from the [Data Science GitHub Data Directory](https://github.com/dsbristol/dst/tree/master/data) using the [Direct Download link](https://github.com/dsbristol/dst/blob/master/data/abcnews-date-text.csv.gz?raw=true)\n",
    "using the python below.\n",
    "\n",
    "We will also get the [dst-block7-lda.zip](https://github.com/dsbristol/dst/blob/master/data/dst-block7-lda.zip?raw=true) zip file which contains some intermediate results that we don't want to have to run in real time. They take about 10-15 mins to regenerate in total and you will have the code to generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/dsbristol/dst/blob/master/data/abcnews-date-text.csv.gz?raw=true'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('../data/abcnews-date-text.csv.gz', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/abcnews-date-text.csv.gz', compression='gzip',error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text[0:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/dsbristol/dst/blob/master/data/dst-block7-lda.zip?raw=true'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('../data/dst-block7-lda.zip', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the file is not corrupted: it should have a hash of 793d48054fa8ec271ed6c683295f1122\n",
    "\n",
    "This is important because the zip files contains pkl files which are vulnerable to malicious use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "print(hashlib.md5(r.content).hexdigest()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll unzip these files so you don't have to regenerate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('../data/dst-block7-lda.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it, we're ready to do NLP in anger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
