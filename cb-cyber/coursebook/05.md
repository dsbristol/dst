---
title: 5.0 Supervised Learning and Ensembles
layout: coursebook-cyber
---
# 05 Supervised Learning and Ensembles

In block 05 we cover:

* Introduction to Classification:
  - Classification in context
  - Logistic Regression
  - Receiver-Operator and Precision-Recall Curves (ROC and PR Curves)
  - Area Under the Curve
  * k-Nearest Neighbour (kNN) classification
* Linear Discriminant Analysis (the 'first' LDA)
* Support Vector Machines (SVM)
* Ensemble methods/Meta-methods, including:
  * Bootstrap Aggregating (Bagging)
  * Boosting
  * Stacking

## Lectures:

* Introduction to Classification:
  * [{{ site.data.cyber.vid05.v0101.text }}]({{ site.data.cyber.vid05.v0101.url }})
    * [Slides]({{ site.data.cyber.block05.s0101.url }})
  * [{{ site.data.cyber.vid05.v0102.text }}]({{ site.data.cyber.vid05.v0102.url }})
    * [Slides]({{ site.data.cyber.block05.s0102.url }})
  * [Reference R code]({{ site.data.cyber.block05.c01.url }})
* Ensemble Methods:
  * [{{ site.data.cyber.vid05.v02.text }}]({{ site.data.cyber.vid05.v02.url }})
    * [Slides]({{ site.data.cyber.block05.s02.url }})

## Worksheets:

* [Worksheet 5.1 Non-parametrics]({{ site.data.cyber.block05.ws01.url }}) 
* [Worksheet 5.2 Outliers and Missing Data]({{ site.data.cyber.block05.ws02.url }})

## Workshop:

* [{{ site.data.cyber.vid05.v03.text }}]({{ site.data.cyber.vid05.v03.url }})
  * [Rmd for 5.3 Workshop on Ensemble Methods]({{ site.data.cyber.block05.s03.url }})

## Assessments:

* [Assessment 2]({{ site.data.cyber.assessment2.url }}) will be set in this week; see [Assessments](../assessments.md). This is a summatieve assessment (i.e. does contribute to your grade) and will be due in Week 12.


## References

* General Classification:
  * [Rob Schapire's ML Classification](https://www.cs.princeton.edu/~schapire/talks/picasso-minicourse.pdf) features a Batman Example...
  * Chapter 4 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) 
* ROC and PR:
  * [Stack Exchange Discussion of ROC vs PR curves](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves).
  * [Davis and Goadrich, "The Relationship Between Precision-Recall and ROC Curves"](https://www.biostat.wisc.edu/~page/rocpr.pdf), ICML 2006.
(Friedman, Hastie and Tibshirani).
* k-Nearest Neighbours:
  - Chapter 13.3 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
* Linear Discriminant Analysis:
  - [Sebastian Raschka's PCA vs LDA article with Python Examples](https://sebastianraschka.com/Articles/2014_python_lda.html#principal-component-analysis-vs-linear-discriminant-analysis)
  - Chapter 4.3 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
* SVMs:
  - [Jason Weston's SVMs tutorial](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf)
  - [e1071 Package for SVMs in R](ftp://ftp.cse.yzu.edu.tw/CRAN/web/packages/e1071/vignettes/svmdoc.pdf)
  - Chapter 12 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
* Ensemble learning in general:
	* [Vadim Smolyakov, MIT: ML-perspective on Ensemble Methods](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)
	* [Stacked Ensembles by H2O](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html), a Commercial AI Company focussing on Deployable AI
	* [StackExchange: Stacking vs Bagging vs Boosting](https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)
	* [Super Learners: van der Laan, M, Polley E, Hubbard A, "Super Learner" (2007)](https://pubmed.ncbi.nlm.nih.gov/17910531/) Statistical Applications in Genetics and Molecular Biology, Volume 6.
* Boosting:
  * [AdaBoost paper: Experiments with a New Boosting Algorithm](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.6252&rep=rep1&type=pdf) Freund and Schapire (1996).
  * [Explaining AdaBoost](http://rob.schapire.net/papers/explaining-adaboost.pdf), Rob Schapire, Empirical Inference (2013) pp 37-52.
  * [xgboost Chen T and Carlos G (2016) KDD 2016](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf).
  * [xgboost explained](https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c), a blog post about Didrik Nilsen's paper [Tree Boosting With XGBoost: Why Does XGBoost Win “Every” Machine Learning Competition?](https://ntnuopen.ntnu.no/ntnu-xmlui/bitstream/handle/11250/2433761/16128_FULLTEXT.pdf)

* Kroese et al's [Data Science & Machine Learning](https://acems.org.au/data-science-machine-learning-book-available-download) free ebook looks pretty helpful.

Previous: [Block 04](04.md).
Next: [Block 06](06.md).
