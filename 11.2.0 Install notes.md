# Install spark and java

Installing a working Spark and/or Hadoop setup is non-trivial. You are encouraged to spend time to get at least one version working: either Bluecrystal phase 4, or a local version.

* [General overview](https://opensource.com/article/18/11/pyspark-jupyter-notebook)
* [Installation on Windows](https://naomi-fridman.medium.com/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f)
* [Installation on Mac](https://medium.com/@roshinijohri/spark-with-jupyter-notebook-on-macos-2-0-0-and-higher-c61b971b5007)

## 1. On Mac OSX (Big Sur)

### 1.1 Install spark, and its dependencies (including java)
```
brew install apache-spark
echo 'export PATH="/opt/homebrew/opt/openjdk@11/bin:$PATH"' >> $HOME/.bash_profile
```

(NB: in the future the openjdk version may change; check it with `ls -d /opt/homebrew/opt/openjdk*`).

To load this path change in your current session, run `source ~/.bash_profile`.

### 1.2 Install pyspark
```
conda install pyspark
```

Now pyspark is ready to use in a notebook.

## 2 On Windows 10

These instructions are followed from [here](https://naomi-fridman.medium.com/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f) but that includes some additional and unneeded steps.

## 2.1. Install Anaconda python

You may already have a functioning Conda setup, but if not, I thoroughly recommend [Miniconda](https://docs.conda.io/en/latest/miniconda.html). You want **Miniconda3 Windows 64-bit** for **Python3** (latest version).

You can then run it by running `Anaconda Prompt (miniconda)` which brings up a **Terminal window** that has some additional functionality over `cmd.exe`.  The commands below are to be run from here.  Test it with:
```
python
```
which should say something like:
```
Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type "help", "copyright", "credits" or "license" for more information.
```
Type `exit()` to quit.

Note that it is possible to download python for Windows from e.g. the Windows Store, but Conda is really good.

### 2.2. Install Java

You must install [Java](https://www.java.com/en/download/) if you have not already done so. Yuo want the **Java SE Development Kit** (though the Runtime Environment will work too) and  **Windows x64** version.

To check that you have Java installed and correctly placed in your [path](https://www.java.com/en/download/help/path.html), run:
```
java -version
```
which should say something like:
```
java version "1.8.0_281"
Java(TM) SE Runtime Environment (build 1.8.0_281-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode)
```

If you have installed Java but it cannot be found, you have to [add it](https://www.java.com/en/download/help/path.html) to your **PATH** (see below). This should not be necessary for Java unless you've done something non-standard, but for reference your path will be something like:
```
C:\Program Files\Java\jre1.8.0_281\bin
```

### 2.3. Install Spark

Download [Spark](http://spark.apache.org/downloads.html). You want the most recent version **built for Hadoop 2.7**.

You can use:
```
cd Downloads
tar -xzvf spark-3.1.1-bin-hadoop2.7.tgz
move spark-3.1.1-bin-hadoop2.7 C:/spark
```
to extract the `tgz` file and move it to `C:/spark`. Otherwise you can use third party software such as [7 zip](https://www.7-zip.org/) which is free and open source.

You then have to **manually add** the correct location to your **PATH** (see below; note that in Windows, `Path` and `PATH` are the same variable because it is not case sensitive), which for me was:
```
C:\spark\bin
```
And **add the following** variables (either as User variables, effecting only your user, or Environment variables effecting all users):
```
SPARK_HOME  = C:\spark
HADOOP_HOME = C:\spark
PYSPARK_DRIVER_PYTHON=jupyter
PYSPARK_PYTHON=python
```

(Thanks to [Stack Overflow](https://stackoverflow.com/questions/56213955/python-worker-failed-to-connect-back-in-pyspark-or-spark-version-2-3-1) for the PYSPARK variable hints...)

#### ADDING A VARIABLE, OR EDITING YOUR PATH

1. Press start, type and select: **Control Panel**
2. In "search control panel", type "environment". Click "Edit environment variables for your account".
3. If you are creating a new variable, click **New...** and enter its name and value.
4. If you are editing an already existing variable, click **Edit...**. You most likely want to create a **New** entry for this variable, though if you make a mistake, you can delete and/or edit.

### 2.3b OPTIONAL: Install Hadoop

You can optionally install Hadoop for Windows. There is little need to do this, but it enables your Spark instance to access HDFS partitions.

See the [Apache Hadoop](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html) page for details.

### 2.4 Testing

You can test the python and spark installation together with:
```
pyspark
```
which should show something like:
```
Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type "help", "copyright", "credits" or "license" for more information.
21/03/24 14:18:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Python version 3.8.5 (default, Sep  3 2020 21:29:08)
Spark context Web UI available at http://CB71353.broadband:4040
Spark context available as 'sc' (master = local[*], app id = local-1616595488807).
SparkSession available as 'spark'.
```
Type `exit()` to quit. You can now launch a Python Notebook with:
```
jupyter notebook
```
And you should be able to run pyspark from the notebook.


## 3 On Bluecrystal 

Accessing spar and java is handled with **Modules**, which are described within the relevent sections. Some downloads are also required for hadoop, which provides HDFS, but this is described using a working version on BC4.

## 4 Accessing the data and code

The basic way to access this code is via:

```{bash}
git clone https://github.com/dsbristol/pyspark.git
```

Then get started with:

* [11.2.1 on Hadoop](11.2.1%20Hadoop%20On%20BC4.sh)
* [11.2.2 on Pyspark in Jupyter](11.2.2%20Pyspark%20from%20Jupyter.ipynb)
* [11.2.3 on Pyspark on BC4](11.2.3%20Pyspark%20on%20BC4.sh)

