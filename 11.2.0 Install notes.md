# Install spark and java

Installing a working Spark and/or Hadoop setup is non-trivial. You are encouraged to spend time to get at least one version working: either Bluecrystal phase 4, or a local version.

* [General overview](https://opensource.com/article/18/11/pyspark-jupyter-notebook)
* [Installation on Windows](https://naomi-fridman.medium.com/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f)
* [Installation on Mac](https://medium.com/@roshinijohri/spark-with-jupyter-notebook-on-macos-2-0-0-and-higher-c61b971b5007)

## On Mac OSX (Big Sur)

### Install spark, and its dependencies (including java)
```
brew install apache-spark
echo 'export PATH="/opt/homebrew/opt/openjdk@11/bin:$PATH"' >> $HOME/.bash_profile
```

To make this change apply to your current session, run `source ~/.bash_profile`.

### Install pyspark
```
conda install pyspark
```

Now pyspark is ready to use in a notebook.

## On Windows 10

These instructions are followed from [here](https://naomi-fridman.medium.com/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f) but that includes some additional and unneeded steps.

## 1. Install Anaconda python

You may already have a functioning Conda setup, but if not, I thoroughly recommend [Miniconda](https://docs.conda.io/en/latest/miniconda.html). You want **Miniconda3 Windows 64-bit** for **Python3** (latest version).

You can then run it by running `Anaconda Prompt (miniconda)` which brings up a **Terminal window** that has some additional functionality over `cmd.exe`.  The commands below are to be run from here.  Test it with:
```
python
```
which should say something like:
```
Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type "help", "copyright", "credits" or "license" for more information.
```
Type `exit()` to quit.

Note that it is possible to download python for Windows from e.g. the Windows Store, but Conda is really good.

### 2. Install Java

You must install [Java](https://www.java.com/en/download/) if you have not already done so. Yuo want the **Java SE Development Kit** (though the Runtime Environment will work too) and  **Windows x64** version.

To check that you have Java installed and correctly placed in your [path](https://www.java.com/en/download/help/path.html), run:
```
java -version
```
which should say something like:
```
java version "1.8.0_281"
Java(TM) SE Runtime Environment (build 1.8.0_281-b09)
Java HotSpot(TM) 64-Bit Server VM (build 25.281-b09, mixed mode)
```

If you have installed Java but it cannot be found, you have to [add it](https://www.java.com/en/download/help/path.html) to your **PATH**. This should not be necessary for Java unless you've done something non-standard, but for reference your path will be something like:
```
C:\Program Files\Java\jre1.8.0_281\bin
```

### 3. Install Spark

Download [Spark](http://spark.apache.org/downloads.html). You want the most recent version **built for Hadoop 2.7**.

You can use:
```
cd Downloads
tar -xzvf spark-3.1.1-bin-hadoop2.7.tgz
move spark-3.1.1-bin-hadoop2.7 C:/spark
```
to extract the `tgz` file and move it to `C:/spark`. Otherwise you can use third party software such as [7 zip](https://www.7-zip.org/) which is free and open source.

You then have to **manually add** the correct location to your **PATH** (see below; note that in Windows, `Path` and `PATH` are equivelent because it is not case sensitive), which for me was:
```
C:\spark\bin
```
And **add the following** variables:
```
SPARK_HOME  = C:\spark
HADOOP_HOME = C:\spark
```

#### ADDING A VARIABLE, OR EDITING YOUR PATH

1. Press start, type and select: **Control Panel**
2. In "search control panel", type "environment". Click "Edit environment variables for your account".
3. If you are creating a new variable, click **New...** and enter its name and value.
4. If you are editing an already existing variable, click **Edit...**. You most likely want to create a **New** entry for this variable, though if you make a mistake, you can delete and/or edit.

### 4 Testing

You can test the python and spark installation together with:
```
pyspark
```
which should show something like:
```
Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type "help", "copyright", "credits" or "license" for more information.
21/03/24 14:18:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/

Using Python version 3.8.5 (default, Sep  3 2020 21:29:08)
Spark context Web UI available at http://CB71353.broadband:4040
Spark context available as 'sc' (master = local[*], app id = local-1616595488807).
SparkSession available as 'spark'.
```
Type `exit()` to quit. You can now launch a Python Notebook with:
```
jupyter notebook
```
And you should be able to run pyspark from the notebook.


## On Bluecrystal 

Accessing spar and java is handled with **Modules**, which are described within the relevent sections. Some downloads are also required for hadoop, which provides HDFS, but this is described using a working version on BC4.

## Accessing the data and code

The basic way to access this code is via:

```{bash}
git clone https://github.com/dsbristol/pyspark.git
```

Then get started with:

* [11.2.1 on Hadoop](1.2.1 Hadoop On BC4.sh)
* [11.2.2 on Pyspark in Jupyter](11.2.2 Pyspark from Jupyter.ipynb)
* [11.2.3 on Pyspark on BC4](11.2.3 Pyspark on BC4.sh)
