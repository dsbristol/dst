{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3.1 Workshop on Topic Modelling (Part 1, Prereqs)\n",
    "\n",
    "```\n",
    "date: \"Block 08\"\n",
    "author: \"Daniel Lawson\"\n",
    "email: dan.lawson@bristol.ac.uk\n",
    "output: html_document\n",
    "version: 2.0.0\n",
    "```\n",
    "\n",
    "## NLP Environment\n",
    "\n",
    "Here we set up the NLP environment in python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The libraries that are needed\n",
    "\n",
    "The main library for this is called \"gensim\". However, there are several libraries that implement better natural language processing technologies.  The main facility is something called \"stemming\" or \"lemmatizing\". \n",
    "\n",
    "https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "\n",
    "* **Stemming**: Cutting off the start and end of words to make (hopefully) consistent word stems\n",
    "* **Lemmatization**: Looking up the correct word-roots in dictionaries to find the morphological root of words\n",
    "\n",
    "Stemming may work across languages and dialects, and is computationally cheaper. However, it may merge words that are functionally different; for example, *operation*, *operational*, *operand* and *opera* might all become *oper*!\n",
    "\n",
    "Lemmatization works only as well as its dictionary and requires large databases to function. It would be preferred for many language tasks.\n",
    "\n",
    "An important additional step is to remove **stop words**. These are common words that link other words but have little intrinsic meaning by themselves, such as \"the\", \"it\", \"on\", \"and\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of software\n",
    "\n",
    "Here we install the required packages, nltk and gensim. Note that this is not completing the setup for these as they contain sub-modules that need configuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (3.8)\n",
      "Requirement already satisfied: tqdm in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: gensim in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: pyLDAvis in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (1.1.3)\n",
      "Requirement already satisfied: future in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: gensim in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (4.2.0)\n",
      "Requirement already satisfied: sklearn in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (0.0.post1)\n",
      "Requirement already satisfied: scipy in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (1.9.1)\n",
      "Requirement already satisfied: joblib in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (1.5.0)\n",
      "Requirement already satisfied: setuptools in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (65.4.1)\n",
      "Requirement already satisfied: funcy in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: numexpr in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from pandas>=1.2.0->pyLDAvis) (2022.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/madjl/envs/jupyter/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check our gensim version.\n",
    "\n",
    "You can ignore this; but, you need gensim v3, not v0, for some of the functions to work. \n",
    "\n",
    "The command-line update is \"conda install -c anaconda gensim\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to test that we can load all software we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import requests\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.0\n"
     ]
    }
   ],
   "source": [
    "print (gensim.__version__)\n",
    "## If you need to reinstall and reload:\n",
    "##from importlib import reload\n",
    "##reload(gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to download the nltk modules that are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/madjl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/madjl/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/madjl/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Do this once! Then leave commented next time you run the script.\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('popular') # Get the \"popular\" package with the advanced WordNet dictionary in it. There are ways to avoid this is you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, load the data. This idea comes from [Susan Li on Towards Data Science](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) and the data is direct from [Kaggle million headlines](https://www.kaggle.com/therohk/million-headlines/data). Because the documents are short, this is a useful dataset for teaching.\n",
    "\n",
    "We will download it from the [Data Science GitHub Data Directory](https://github.com/dsbristol/dst/tree/master/data) using the [Direct Download link](https://github.com/dsbristol/dst/blob/master/data/abcnews-date-text.csv.gz?raw=true)\n",
    "using the python below.\n",
    "\n",
    "We will also get the [dst-block7-lda.zip](https://github.com/dsbristol/dst/blob/master/data/dst-block7-lda.zip?raw=true) zip file which contains some intermediate results that we don't want to have to run in real time. They take about 10-15 mins to regenerate in total and you will have the code to generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19296792"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "url = 'https://github.com/dsbristol/dst/blob/master/data/abcnews-date-text.csv.gz?raw=true'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open(os.path.join('..', 'data', 'abcnews-date-text.csv.gz'), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/6b6ndf8s69l3glyl3t0xl7zr0000gq/T/ipykernel_47717/3200708286.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(os.path.join('..', 'data', 'abcnews-date-text.csv.gz'),\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join('..', 'data', 'abcnews-date-text.csv.gz'), \n",
    "                   compression='gzip',error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text[0:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6239995"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/dsbristol/dst/blob/master/data/dst-block7-lda.zip?raw=true'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open(os.path.join('..', 'data', 'dst-block7-lda.zip'), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the file is not corrupted: it should have a hash of 793d48054fa8ec271ed6c683295f1122\n",
    "\n",
    "This is important because the zip files contains pkl files which are vulnerable to malicious use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793d48054fa8ec271ed6c683295f1122\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "print(hashlib.md5(r.content).hexdigest()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll unzip these files so you don't have to regenerate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(os.path.join('..', 'data', 'dst-block7-lda.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join('..', 'data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it, we're ready to do NLP in anger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
