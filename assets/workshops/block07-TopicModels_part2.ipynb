{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop on Topic Modelling (Part 2, Main Content)\n",
    "\n",
    "```\n",
    "date: \"Block 07\"\n",
    "author: \"Daniel Lawson\"\n",
    "email: dan.lawson@bristol.ac.uk\n",
    "output: html_document\n",
    "version: 1.0.1\n",
    "```\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Topic models can be applied to cyber security data. See for examples:\n",
    "\n",
    "* For a general discussion of threat detection using NLP, see e.g. https://www.endgame.com/blog/technical-blog/nlp-security-malicious-language-processing\n",
    "* For a concrete example, see https://github.com/python-security/pyt\n",
    "\n",
    "However, finding data appropriate to them is more difficult. Additionally, traditional natural language processing also has application in cyber security. Examples:\n",
    "\n",
    "* Profiling Underground Economy Sellers\n",
    "* Understanding Hacker Source Code \n",
    "\n",
    "were given in \"[Topic Modeling and Latent Dirichlet Allocation: An Overview](https://ai.arizona.edu/sites/ai/files/MIS611D/lda.pptx)\" (Weifeng Li, Sagar Samtani and Hsinchun Chen Acknowledgements: David Blei, Princeton University, The Stanford Natural Language Processing Group) \n",
    "\n",
    "Further [Bobby Filar describes NLP For Security: Malicious Language Processing](https://www.endgame.com/blog/technical-blog/nlp-security-malicious-language-processing) which explains the following areas:\n",
    "\n",
    "* [Domain Generation Algorithm classification](http://conferences.sigcomm.org/imc/2010/papers/p48.pdf) – Using NLP to identify malicious domains (e.g., blbwpvcyztrepfue.ru) from benign domains (e.g., cnn.com)\n",
    "* [Source Code Vulnerability Analysis](https://www.usenix.org/legacy/events/woot11/tech/slides/yamaguchi.pdf) – Determining function patterns associated with known vulnerabilities, then using NLP to identify other potentially vulnerable code segments.\n",
    "* [Phishing Identification](http://nlp.uned.es/~lurdes/araujo/eswa13_malicious_tweets.pdf) – A bag-of-words model determines the probability an email message contains a phishing attempt or not.\n",
    "* [Malware Family Analysis](https://www.endgame.com/blog/examining-malware-python) –Topic modeling techniques assign samples of malware to families.\n",
    "\n",
    "However, none of these contain data. So we will go over a traditional text-based NLP in this workshop.\n",
    "\n",
    "Additional references:\n",
    "A [Gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) overview, and a description of \n",
    "[Coherence](https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, load the data. This idea comes from [Susan Li on Towards Data Science](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) and the data is direct from [Kaggle million headlines](https://www.kaggle.com/therohk/million-headlines/data).\n",
    "\n",
    "A reminder: We downloaded this in Part 1, from the [DSBristol github](https://github.com/dsbristol/dst/tree/master/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/abcnews-date-text.csv.gz', compression='gzip',error_bad_lines=False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text[0:100000]\n",
    "print(len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summaries of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines THREE choices of lemmatizer:\n",
    "* \"normalise_text\" uses manual wordnet lemmatisation. It tries to use word position to figure out whether something is and adjective, verb, noun or adverb. \n",
    "* \"preprocess\" doesn't bother with that, it uses a standard lemmatizer.\n",
    "* \"prepare_text_for_lda\" also lemmatizes, but it also handles stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Needed for stop words (only)\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lmtzr = nltk.WordNetLemmatizer().lemmatize\n",
    "\n",
    "## We lookup whether a word is and adjective, verb, noun or adverb here.\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    \n",
    "## This version uses word type. Needs the bigger nltp download (\"popular\")\n",
    "def normalize_text(text):\n",
    "    ## Runs on documents (vector of words)\n",
    "    word_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
    "\n",
    "    return [x.lower() for x in lemm_words]\n",
    "\n",
    "## This version doesn't require the \"popular\" download\n",
    "def preprocess(text):\n",
    "    ## Runs on documents (vector of words)\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    return([lemmatizer.lemmatize(i) for i in text.split()])\n",
    "\n",
    "################\n",
    "## wordnet version\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    ## morphy does a lemma lookup and word standardization\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "## lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "## This version is for comparison\n",
    "def prepare_text_for_lda(text):\n",
    "    ## Runs on documents (vector of words)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying these to example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[documents['index'] == 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import parsing\n",
    "doc_sample = documents[documents['index'] == 16].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(normalize_text(doc_sample))\n",
    "print('\\n\\n simpler tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "print('\\n\\n method removing stop words: ')\n",
    "print(prepare_text_for_lda(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this to the dataset as a whole. (warning: takes a little time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess) # preprocess is faster than normalise_text.\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll make a dictionary and report some of the items in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k,v  in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to get rid of extremes. This is one way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a corpus\n",
    "Now we will map the documents into the bag of words model. As you can see, a corpus is simply a list of documents, each of which is a list of words.\n",
    "\n",
    "However, creating them is slow enough so that you might want to download them preprocessed from [dst-block7-lda.zip](https://github.com/dsbristol/dst/blob/master/data/dst-block7-lda.zip?raw=true). (Downloaded in part1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Reading corpus from pickle\")\n",
    "    bow_corpus=pickle.load(open('../data/bow_corpus.pkl', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating corpus and saving to pickle\")\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    pickle.dump(bow_corpus, open('../data/bow_corpus.pkl', 'wb'))\n",
    "    pickle.dump(dictionary, open('../data/dictionary.pkl', 'wb'))\n",
    "\n",
    "bow_corpus[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_16 = bow_corpus[1000]\n",
    "\n",
    "for i in range(len(bow_doc_16)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_16[i][0], \n",
    "                                               dictionary[bow_doc_16[i][0]], \n",
    "                                                bow_doc_16[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a version with stop words removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs2 = documents['headline_text'].map(prepare_text_for_lda) \n",
    "processed_docs2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2 = gensim.corpora.Dictionary(processed_docs2)\n",
    "\n",
    "count2 = 0\n",
    "for k, v in dictionary2.iteritems():\n",
    "    print(k, v)\n",
    "    count2 += 1\n",
    "    if count2 > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary2.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the corpus\n",
    "\n",
    "The code below **remakes** the corpus, simply by looping over the words in the document that are in the reduced dictionary, and mapping them to their sparse vector notation.\n",
    "\n",
    "However, this is quite slow, so we instead check whether it was pre-created and saved to file. If so, we read it from file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Reading corpus from pickle...\")\n",
    "    bow_corpus2=pickle.load(open('../data/bow_corpus2.pkl', 'rb'))\n",
    "except FileNotFoundError:\n",
    "    print(\"Reading corpus failed.\")\n",
    "    print(\"Creating corpus and saving to pickle\")\n",
    "    bow_corpus2 = [dictionary2.doc2bow(doc) for doc in processed_docs2]\n",
    "    pickle.dump(bow_corpus2, open('../data/bow_corpus2.pkl', 'wb'))\n",
    "    pickle.dump(dictionary2, open('../data/dictionary2.pkl', 'wb'))\n",
    "\n",
    "bow_corpus2[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an LDA model\n",
    "\n",
    "This is the key component of an LDA model: defining the model with a specified corpus and dictionary.\n",
    "\n",
    "Note that we also have to specify how many topics we will generate as well as the number of passes through the data. Because the inference algorithm is sensitive to word order, we can get different answers when rerunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lda_model=pickle.load(open('../data/lda_model.pkl', 'rb'))\n",
    "    print(\"Reading lda_model from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_model and saving to pickle\")\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "    pickle.dump(lda_model,open('../data/lda_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lda_model2=pickle.load(open('../data/lda_model2.pkl', 'rb'))\n",
    "    print(\"Reading lda_model2 from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_model2 and saving to pickle\")\n",
    "    lda_model2 = gensim.models.LdaMulticore(bow_corpus2, num_topics=10, id2word=dictionary2, passes=2, workers=2)\n",
    "    pickle.dump(lda_model2,open('../data/lda_model2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll explore the model a little.\n",
    "\n",
    "First we compare a document to its topic representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents['headline_text'][89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model[bow_corpus[89]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the text a little to see how the topics change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='woman fined after aboriginal tent embassy raid'\n",
    "pptext=preprocess(text)\n",
    "lda_model[dictionary.doc2bow(pptext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='badger fined after aboriginal tent embassy raid'\n",
    "pptext=preprocess(text)\n",
    "lda_model[dictionary.doc2bow(pptext)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='man fined after badger tent embassy raid'\n",
    "pptext=preprocess(text)\n",
    "lda_model[dictionary.doc2bow(pptext)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics(20,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf model\n",
    "\n",
    "Now we rerun with tf-idf.\n",
    "\n",
    "We keep the exact same dictionaries, but use the tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lda_model_tfidf=pickle.load(open('../data/lda_model_tfidf.pkl', 'rb'))\n",
    "    print(\"Reading lda_model_tfidf from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_model_tfidf and saving to pickle\")\n",
    "    lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "    pickle.dump(lda_model,open('../data/lda_model_tfidf.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf.show_topics(10,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = models.TfidfModel(bow_corpus2)\n",
    "corpus_tfidf2 = tfidf[bow_corpus2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lda_model_tfidf2=pickle.load(open('../data/lda_model_tfidf2.pkl', 'rb'))\n",
    "    print(\"Reading lda_model_tfidf2 from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_model_tfidf2 and saving to pickle\")\n",
    "    lda_model_tfidf2 = gensim.models.LdaMulticore(corpus_tfidf2, num_topics=10, id2word=dictionary2, passes=2, workers=4)\n",
    "    pickle.dump(lda_model2,open('../data/lda_model_tfidf2.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing on out-of-sample data\n",
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "#unseen_document='american coppers found eating donuts once again'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "bow_vector2 = dictionary2.doc2bow(prepare_text_for_lda(unseen_document))\n",
    "print(lda_model2[bow_vector2])\n",
    "print(lda_model_tfidf2[bow_vector2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "The pyLDAvis package has a nice interactive visualisation designed for gensim.\n",
    "\n",
    "We have to prepare the data, which is again a bit slow so I provide the pkl versions of these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lda_display=pickle.load(open('../data/lda_display.pkl', 'rb'))\n",
    "    print(\"Reading lda_display from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_display and saving to pickle\")\n",
    "    lda_display = pyLDAvis.gensim.prepare(lda_model, bow_corpus, \n",
    "                                          dictionary, mds='mmds')\n",
    "    pickle.dump(lda_display,open('../data/lda_display.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lda_display2=pickle.load(open('../data/lda_display2.pkl', 'rb'))\n",
    "    print(\"Reading lda_display2 from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_display2 and saving to pickle\")\n",
    "    lda_display2 = pyLDAvis.gensim.prepare(lda_model2, bow_corpus2, \n",
    "                                          dictionary2, mds='mmds')\n",
    "    pickle.dump(lda_display2,open('../data/lda_display2.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    lda_display_tfidf2=pickle.load(open('../data/lda_display_tfidf2.pkl', 'rb'))\n",
    "    print(\"Reading lda_display_tfidf2 from pickle\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating lda_display_tfidf2 and saving to pickle\")\n",
    "    lda_display_tfidf2 = pyLDAvis.gensim.prepare(lda_model_tfidf2, \n",
    "                                                 corpus_tfidf2, dictionary2, sort_topics=False)\n",
    "    pickle.dump(lda_display_tfidf2,open('../data/lda_display_tfidf2.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use visualisation, found at many places including:\n",
    "\n",
    "https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB under some circumstances you need show, under others you need display. It appears to be a known bug.\n",
    "pyLDAvis.display(lda_display, template_type='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_display2, template_type='notebook') # NB under some circumstances you need show, under others you need display. It appears to be a known bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualise the tfidf2 model, which \"should\" be our best model.\n",
    "\n",
    "What can we learn about the topics here that is different to the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_display_tfidf2, template_type='notebook') # NB under some circumstances you need show, under others you need display. It appears to be a known bug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return to topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model2[bow_vector2], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model2.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model2[bow_vector2], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model2.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model_tfidf2[bow_vector2], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf2.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity and Coherence\n",
    "\n",
    "Question: What does a \"good\" model prediction look like?\n",
    "\n",
    "The below examines the scores that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sadly very slow, so we only look at the first few documents.\n",
    "log_perplexities = {\"lda_model\": lda_model.log_perplexity(bow_corpus[0:1000]), \n",
    "     \"lda_model2\" : lda_model2.log_perplexity(bow_corpus2[0:1000]),\n",
    "     \"lda_model_tfidf\" : lda_model_tfidf.log_perplexity(corpus_tfidf[0:1000]),\n",
    "     \"lda_model_tfidf2\" : lda_model_tfidf2.log_perplexity(corpus_tfidf2[0:1000])\n",
    "    };\n",
    "# a measure of how good the model is. lower the better.\n",
    "log_perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why is the tf-idf model so much better in this measure? Does the performance measure capture your intuition about what a good topic model is?\n",
    "\n",
    "Now we compute the intrinsic coherence to check the quality of the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "def getCoherence(m,c,d):\n",
    "    coherence_model_lda = CoherenceModel(model=m,corpus=c, dictionary=d, coherence='u_mass')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return(coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute Coherence Score\n",
    "coherences={\n",
    "    \"lda_model\": getCoherence(lda_model,bow_corpus[0:1000],dictionary),\n",
    "    \"lda_model2\": getCoherence(lda_model2,bow_corpus2[0:1000],dictionary2),\n",
    "    \"lda_model_tfidf\": getCoherence(lda_model_tfidf,corpus_tfidf[0:1000],dictionary),\n",
    "    \"lda_model_tfidf2\": getCoherence(lda_model_tfidf2,corpus_tfidf2[0:1000],dictionary2)\n",
    "}\n",
    "# a different measure of how good the model is. Higher is better.\n",
    "coherences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Why is the version of the data in which we removed stop words performing worse? \n",
    "\n",
    "## Return to out-of-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_coherences={\n",
    "    \"lda_model\": getCoherence(lda_model,[bow_vector],dictionary),\n",
    "    \"lda_model2\": getCoherence(lda_model2,[bow_vector2],dictionary2),\n",
    "    \"lda_model_tfidf\": getCoherence(lda_model_tfidf,[bow_vector],dictionary),\n",
    "    \"lda_model_tfidf2\": getCoherence(lda_model_tfidf2,[bow_vector2],dictionary2)\n",
    "};\n",
    "oos_coherences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: what would be the most appropriate way to test an LDA model? Would it make a difference to possess labels? How would you use them?\n",
    "\n",
    "## Challenge: what is the \"best\" value of K to use? \n",
    "How would you evaluate it? How would you handle the model runtime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "What conclusions would you draw from this procedure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "NLTK includes synonyms, dictionary definitions, antonyms, and more; all available for automated processing.\n",
    "\n",
    "Some tasters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets(\"pain\")\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "for syn in wordnet.synsets('Computer'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
