---
title: 07 Topic Models and Bayes
layout: coursebook
---
# 07 Topic Models and Bayes

In this block we cover:

* Simple topic models
  * The Bag Of Words
  * term frequency, inverse document frequency (tf-idf) representation
  * N-grams
* Navigating Bayesian Methodology
	* Bayes Theorem
	* Bayesian Motivations for Smoothing and Regularisation
	* False Positive Rates
	* How MCMC, SMC, ABC and Variational Inference fit together
* Latent Dirichlet Allocation
  * Perplexity and coherence
  * Text cleaning
* Practical use of LDA
  * Regexp (regular expressions)
  * Pipelines for data processing
  * Example application
  
## Lectures:

* Topic Models, Bayes, Regularization, Latent Dirichlet Allocation:
  * [{{ site.data.vid07.v0101.text }}]({{ site.data.vid07.v0101.url }})
    * [Slides]({{ site.data.block07.s0101.url }})
  * [{{ site.data.vid07.v0102.text }}]({{ site.data.vid07.v0102.url }})
    * [Slides]({{ site.data.block07.s0102.url }})
  * [{{ site.data.vid07.v0103.text }}]({{ site.data.vid07.v0103.url }})
    * [Slides]({{ site.data.block07.s0103.url }})
* Applying Topic Models:
  * [{{ site.data.vid07.v02.text }}]({{ site.data.vid07.v02.url }})
    * [Slides]({{ site.data.block07.s02.url }})

## Worksheets:

* [Worksheet 7.1 Topic Models and Bayes]({{ site.data.block07.ws01.url }}) 

## Workshop:

The workshop is split into two sections. The first of these installs gensim and uses [NLTK (Natural Language Toolkit](https://www.nltk.org/) to install some useful tools. It also gets the data. The second is the serious workshop containing a full text modelling example.

* [{{ site.data.vid07.v0301.text }}]({{ site.data.vid07.v0301.url }})
  * [Python Notebook: 7.3.1 Topic Models (Software and downloading data)]({{ site.data.block07.s0301.url }})
* [{{ site.data.vid07.v0302.text }}]({{ site.data.vid07.v0302.url }})
  * [Python Notebook: 7.3.2 Topic Models]({{ site.data.block07.s0302.url }})


## Assessments:

* [Assessment 3]({{ site.data.assessment3.url }}) will be set in this week; see [Assessments](../assessments.md). This is a summatieve assessment (i.e. does contribute to your grade) and will be due in Week 16.

## References

### Bag of Words

* Python Bag of Words: p259 Python Machine Learning (Raschka & Mirjalili, 2nd ed 2017).
* Topic Modeling and Latent Dirichlet Allocation: An Overview (Weifeng Li, Sagar Samtani and Hsinchun Chen)
* Stephen Robinson, Microsoft Research [Understanding Inverse Document Frequency: On theoretical arguments for IDF](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.7340&rep=rep1&type=pdf)

### Bayesian Methodology

* Bayesian Programming languages: Software which allows you to specify the model without requiring you to specify the method:
  * [OpenBUGS](http://openbugs.net/w/FrontPage)
  * [JAGS](http://mcmc-jags.sourceforge.net/)
  * [STAN](http://mc-stan.org/)
* There is a super useful list of conjugate priors and interpretations on the [Conjugate Prior Wikipedia page](https://en.wikipedia.org/wiki/Conjugate_prior)!
* Monte Carlo:
  * Gamerman and Hedibert. [Markov chain Monte Carlo: stochastic simulation for Bayesian inference](http://www.dme.ufrj.br/mcmc/).
  * Doucet, Godsill, and Andrieu. "[On sequential Monte Carlo sampling methods for Bayesian filtering](https://www.cs.ubc.ca/~arnaud/doucet_godsill_andrieu_sequentialmontecarloforbayesfiltering.pdf)" Statistics and computing 10.3 (2000): 197-208.
  * Andrieu, Doucet, and Holenstein [Particle Markov chain Monte Carlo methods](https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf)
* ABC:
  * Beaumont, Zhang, and Balding. "[Approximate Bayesian computation in population genetics](https://www.genetics.org/content/162/4/2025)." Genetics 162.4 (2002): 2025-2035.
  * Murray, Ghahramani, and MacKay. "[MCMC for doubly-intractable distributions](https://homepages.inf.ed.ac.uk/imurray2/pub/06doubly_intractable/doubly_intractable.pdf)" Proceedings of the 22nd Annual Conference on Uncertainty in Artificial Intelligence (UAI) (2006).
* Variational Inference:
	* Blei and Jordan. "[Variational inference for Dirichlet process mixtures](https://projecteuclid.org/download/pdf_1/euclid.ba/1340371077)", Bayesian analysis 1.1 (2006): 121-143.
	* [A Beginner's Guide to Variational Methods](http://blog.evjang.com/2016/08/variational-bayes.html), by Eric Jang.


### Latent Dirichlet Allocation
* [Topic Modeling and Latent Dirichlet Allocation: An Overview](https://ai.arizona.edu/sites/ai/files/MIS611D/lda.pptx) (Weifeng Li, Sagar Samtani and Hsinchun Chen)
* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "[Latent dirichlet allocation](https://www.jmlr.org/papers/v3/blei03a)", Journal of machine Learning research 3.Jan (2003): 993-1022.
* Neural Networks approaches to document models:
  * [Mathgen](https://thatsmathematics.com/mathgen/) generates random papers
  * [Topic-RNN](https://github.com/dangitstam/topic-rnn) infers a topic model using a Neural Network

### Data science topic modelling
* [Preparing Data for Topic Modelling](https://publish.illinois.edu/commonsknowledge/2017/11/16/preparing-your-data-for-topic-modeling/)
* [NLP for legal documents](https://towardsdatascience.com/nlp-for-topic-modeling-summarization-of-legal-documents-8c89393b1534)
* [Machine-Learning-In-Law github repo](https://github.com/chibueze07/Machine-Learning-In-Law/tree/master)

### Judging topic models
* Chang, Jonathan, Jordan Boyd-Graber, Sean Gerrish, Chong Wang and David M. Blei. 2009. [Reading Tea Leaves: How Humans Interpret Topic Models](http://umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf). NIPS.
* Stevens, Kegelmeyer, Andrzejewsk and Buttler [Exploring Topic Coherence over many models and many topics](https://www.aclweb.org/anthology/D/D12/D12-1087.pdf)

### Data sources
* [Kaggle dataset for fake news](https://www.kaggle.com/nupursh/nlp-in-r-topic-modelling-for-fake-news)
* [Intelligence and Security Informatics Data Sets](https://www.azsecure-data.org/)
* [Vizsec security data collection](https://vizsec.org/data/)
* [Threatminer cyber data with NLP](https://www.threatminer.org/)
* [Phishing data corpus](https://monkey.org/~jose/phishing/phishing-2018)

Previous: [Block 06](06.md).
Next: [Block 08](08.md).
