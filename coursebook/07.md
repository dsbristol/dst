---
title: 7.0 Perceptions and Neural Networks
layout: coursebook
---
# 07 Perceptrons and Neural Networks

In this block we cover:

* Introduction
  * Neurons
  * Single layer perceptron
  * Learning algorithms
* Deep Neural Networks
* Multi layer perceptron and the feed-forward neural network
  * Learning for deep neural networks
  * CNNs and Transformers

## Lectures

* [Neural Nets and the Perceptron]({{ site.data.block07.s01.url }})
* [CNNs and Transformers]({{ site.data.block07.s02.url }})

## Assessments:

* [Assessment 2]({{ site.data.assessment2.url }}) will be set in this week; see [Assessments](../assessments.md). This is a summative assessment (i.e. does contribute to your grade) and will be due in Week 12.
* [Portfolio 07]({{ site.data.block07.portfolio.url }}) of the full [Portfolio]({{ site.data.individualassessment1.url }}).
* **Block07** on Noteable via [Blackboard](https://www.ole.bris.ac.uk/ultra/courses/_255714_1/cl/outline):

## Workshop:

* [Colab Notebook: Workshop 07.3 on Pytorch for Computer Vision](https://colab.research.google.com/drive/1e_YqHCjGoEdoof2uEFodk-2mF0yqnn9O?usp=sharing#scrollTo=V7qhbadrUwcC)
	* [Offline version]({{ site.data.block07.ws03.url }})

## References

### Neural Networks textbooks

* Chapter 11 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
* Russell and Norvig [Artificial Intelligence: A Modern Approach](http://aima.eecs.berkeley.edu/)
  * [Chapter 20 Section 5: Neural Networks](http://aima.eecs.berkeley.edu/slides-pdf/chapter20b.pdf)

### Theoretical practicalities

* Bengio 2012 [Practical Recommendations for Gradient-Based Training of Deep Architectures](http://arxiv.org/pdf/1206.5533.pdf) (in the book "Neural Networks: Tricks of the Trade")
* Kull et al 2019 NeurIPS [Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration](https://papers.nips.cc/paper/2019/file/8ca01ea920679a0fe3728441494041b9-Paper.pdf)
* Swish: Ramachandran, Zoph and Le [Searching for Activation Functions](https://arxiv.org/abs/1710.05941)

### Important historical papers

* McCulloch and Pitts (1943) A logical calculus of the ideas immanent in nervous activity
* Minsky and Papert 1969 Perceptrons
* Hecht-Nielsen, Robert. "Theory of the backpropagation neural network." Neural networks for perception. Academic Press, 1992. 65-93.
* Bishop 1994 [Mixture Density Networks](https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf)

### Likelihood and modelling applications of Neural Networks

* Chilinski and Silva [Neural Likelihoods via Cumulative Distribution Functions](https://arxiv.org/abs/1811.00974)
* Albawi, Mohammed and Al-Zawi [Understanding of a convolutional neural network](https://ieeexplore.ieee.org/abstract/document/8308186?casa_token=WkNQpcZQeX0AAAAA:KJW4xHL-5qc50yzHivHG2f4pnx23A17c3QtIB9PiNlPXxJzFhKn79UUvjnryqiC4__DfeYe8cPE)
* Omi, Ueda and Aihara [Fully Neural Network based Model for GeneralTemporal Point Processes](https://arxiv.org/pdf/1905.09690.pdf)

### Implementations and Examples

* [Neural Network Models in R](https://www.datacamp.com/community/tutorials/neural-network-models-r)
* [Installing Tensoflow](https://www.tensorflow.org/install/pip#windows-native)

### Worksheets (unassessed)

* [Worksheet 7.1 Neural Networks]({{ site.data.block07.ws01.url }}) 

### Historical contents

* Note that these have been superseded by the above lectures and workshop.
	* [7.1 Neural Nets and the Perceptron](https://dsbristol.github.io/dst/dst/assets/slides/07.1-PerceptronsNeuralNets.pdf)
	* [7.2 Neural Net Practicalities](https://dsbristol.github.io/dst/dst/assets/slides/07.2-PracticalitiesOfNeuralNets.pdf)
	* [7.3 Workshop on Keras and Tensorflow](https://github.com/dsbristol/dst/blob/master/dst/assets/workshops/block07-NeuralNetworks.ipynb)

## Navigation:

Previous: [Block 06](06.md).
Next: [Block 08](08.md).

