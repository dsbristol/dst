---
title: 7.0 Perceptions and Neural Networks
layout: coursebook
---
# 07 Perceptions and Neural Networks

In this block we cover:

* Introduction
  * Neurons
  * Single layer perceptron
  * Learning algorithms
* Deep Neural Networks
* Multi layer perceptron and the feed-forward neural network
  * Learning for deep neural networks
  * Other types of neural networks and their value:
	* Feed-forward
	* Convolutional
	* Recurrent
	* Recursive
	* Auto-encoders

## Lectures

* [Neural Nets and the Perceptron]({{ site.data.block07.s01.url }})
* [Practicalities of Neural Nets]({{ site.data.block07.s02.url }})

## Worksheets:

* [Worksheet 7.1 Neural Networks]({{ site.data.block07.ws01.url }}) 
* [Portfolio 07]({{ site.data.block07.portfolio.url }})

## Assessments:

* [Assessment 2]({{ site.data.assessment2.url }}) will be set in this week; see [Assessments](../assessments.md). This is a summative assessment (i.e. does contribute to your grade) and will be due in Week 12.

## Workshop:

* [Python Notebook: Workshop 07.3 on Tensorflow with Keras]({{ site.data.block07.s03.url }})
	* [Neural Network run as a script.]({{ site.data.block07.s0302.url }})
  * You should understand how to run python as a script.
	* This is run (for me) with `python block07-NeuralNetworksScript.py`
	* Remember to put the **data** and **python script** in the directory you are running in!

## References

### Neural Networks textbooks

* Chapter 11 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
* Russell and Norvig [Artificial Intelligence: A Modern Approach](http://aima.eecs.berkeley.edu/)
  * [Chapter 20 Section 5: Neural Networks](http://aima.eecs.berkeley.edu/slides-pdf/chapter20b.pdf)

### Theoretical practicalities
* Bengio 2012 [Practical Recommendations for Gradient-Based Training of Deep Architectures](http://arxiv.org/pdf/1206.5533.pdf) (in the book "Neural Networks: Tricks of the Trade")
* Kull et al 2019 NeurIPS [Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration](https://papers.nips.cc/paper/2019/file/8ca01ea920679a0fe3728441494041b9-Paper.pdf)
* Swish: Ramachandran, Zoph and Le [Searching for Activation Functions](https://arxiv.org/abs/1710.05941)

### Important historical papers:
* McCulloch and Pitts (1943) A logical calculus of the ideas immanent in nervous activity
* Minsky and Papert 1969 Perceptrons
* Hecht-Nielsen, Robert. "Theory of the backpropagation neural network." Neural networks for perception. Academic Press, 1992. 65-93.
* Bishop 1994 [Mixture Density Networks](https://publications.aston.ac.uk/373/1/NCRG_94_004.pdf)

### Likelihood and modelling applications of Neural Networks:

* Chilinski and Silva [Neural Likelihoods via Cumulative Distribution Functions](https://arxiv.org/abs/1811.00974)
* Albawi, Mohammed and Al-Zawi [Understanding of a convolutional neural network](https://ieeexplore.ieee.org/abstract/document/8308186?casa_token=WkNQpcZQeX0AAAAA:KJW4xHL-5qc50yzHivHG2f4pnx23A17c3QtIB9PiNlPXxJzFhKn79UUvjnryqiC4__DfeYe8cPE)
* Omi, Ueda and Aihara [Fully Neural Network based Model for GeneralTemporal Point Processes](https://arxiv.org/pdf/1905.09690.pdf)

### Implementations and Examples

* [Neural Network Models in R](https://www.datacamp.com/community/tutorials/neural-network-models-r)
* [Installing Tensoflow](https://www.tensorflow.org/install/pip#windows-native)

Previous: [Block 06](06.md).
Next: [Block 08](08.md).

