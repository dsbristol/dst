---
title: 4.0 Non-parametrics and Missing Data
layout: coursebook
---
# 04 Non-parametrics and Missing Data

In Block 4 we cover:

* Non-parametric statistics:
  * Transforms and their uses:
    - Fourier Transform
	- Hadamard Transform
  * Kernel Density Estimation
  * k-Nearest Neighbour Density Estimation
  * The Kernel Trick
  * Kernel PCA as an example of the Kernel Trick
* Handling Outliers and Missing Data:
  * Outlier detection and/or removal
  * Robust algorithms
  * Classes of Missing data
  * Approaches for filtering data based on missingness
  * Approaches for imputing missing data
  * How to undersstand the consequences of these choices

## Lectures:

* [04.1 Non-parametric Statistics]({{ site.data.block04.s01.url }})
  * [Reference R code]({{ site.data.block04.c01.url }})
* Missing Data:
  * [04.2.1 Outliers and Missing data (Part 1, Outliers)]({{ site.data.block04.s0201.url }})
  * [04.2.2 Outliers and Missing data (Part 2, Missing Data)]({{ site.data.block04.s0202.url }})
  * [Reference R code]({{ site.data.block04.c02.url }})
  
## Worksheets:

* [Worksheet 4.1 Non-parametrics]({{ site.data.block04.ws01.url }}) 
* [Worksheet 4.2 Outliers and Missing Data]({{ site.data.block04.ws02.url }})
* [Portfolio 04]({{ site.data.block04.portfolio.url }})

## Workshop:

* [04.3 Workshop on Missing Data (.Rmd)]({{ site.data.block04.s03.url }})

## Reference material:

### Non-parametric Statistics:

* Transforms:
  * [Nonparametric Statistics by Eduardo García Portugués](https://bookdown.org/egarpor/NP-UC3M/)
  * Basis Expansions: Chapter 5 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
* Density Estimation:
  * Kernel Smoothing: Chapter 6 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
  * For kNN [Yen-Chi Chen's notes on kNN and the Basis](http://faculty.washington.edu/yenchic/18W_425/Lec7_knn_basis.pdf)
* The Kernel Trick and its applications:
  * For the Kernel Trick [Dave Krebs' Intro to Kernels](https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/class-kernels.pdf)
  * For the Kernel PCA: [Rita Osadchi's Kernel PCA notes](http://www.cs.haifa.ac.il/~rita/uml_course/lectures/KPCA.pdf)
  * Hofmann, Schoelkopf, & Smola (2008) ["Kernel Methods in Machine Learning"](https://www.ccs.neu.edu/home/vip/teach/MLcourse/6_SVM_kernels/materials/0701907.pdf) (Ann. Stat.)
  * Schoelkopf B., A. Smola, K.-R. Mueller (1998) ["Nonlinear component analysis as a kernel eigenvalue problem"](https://www.mlpack.org/papers/kpca.pdf).
* Outlier detection:
  * ["A Survey of Outlier Detection Methodologies"](https://link.springer.com/article/10.1023/B:AIRE.0000045502.10941.a9) by Victoria Hodge & Jim Austin, Artificial Intelligence Review 22:85–126 (2004).
  * [Outlier Analysis by Charu C. Aggarwal](https://link.springer.com/chapter/10.1007/978-3-319-14142-8_8). NB: Not freely available.
  * Chapter 10 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani) discusses the robustness to outliers for various methods.

## Missing data:
  * Chapter 9.6 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
  * I would recommend [Andrew Gelman's Missing Data Notes](http://www.stat.columbia.edu/~gelman/arm/missing.pdf).
  
## Navigation

Previous: [Block 03](03.md).
Next: [Block 05](05.md).
