---
title: 08 Topic Models and Bayes
layout: coursebook
---
# 08 Topic Models and Bayes

In this block we cover:

* Simple topic models
  * The Bag Of Words
  * term frequency, inverse document frequency (tf-idf) representation
  * N-grams
* Navigating Bayesian Methodology
	* Bayes Theorem
	* Bayesian Motivations for Smoothing and Regularisation
	* False Positive Rates
	* How MCMC, SMC, ABC and Variational Inference fit together
* Latent Dirichlet Allocation
  * Perplexity and coherence
  * Text cleaning
* Practical use of LDA
  * Regexp (regular expressions)
  * Pipelines for data processing
  * Example application
  
## Lectures:

* [08.1 A Brief primer in Bayesian methods]({{ site.data.block08.s01.url }})
* [08.2 Topic Models and Latent Dirichlet Allocation]({{ site.data.block08.s02.url }})

## Workshop:

The workshop is split into two sections. The first of these installs gensim and uses [NLTK (Natural Language Toolkit](https://www.nltk.org/) to install some useful tools. It also gets the data. The second is the serious workshop containing a full text modelling example.

* [Python Notebook: 8.3.1 Topic Models (Software and downloading data)]({{ site.data.block08.s0301.url }})
* [Python Notebook: 8.3.2 Topic Models]({{ site.data.block08.s0302.url }})

## Assessments:

* [Portfolio 08]({{ site.data.block08.portfolio.url }}) of the full [Portfolio]({{ site.data.individualassessment1.url }}).
* **Block08** on Noteable via [Blackboard](https://www.ole.bris.ac.uk/ultra/courses/_255714_1/cl/outline):

## References:

### Bag of Words

* Python Bag of Words: p259 Python Machine Learning (Raschka & Mirjalili, 2nd ed 2017).
* Topic Modeling and Latent Dirichlet Allocation: An Overview (Weifeng Li, Sagar Samtani and Hsinchun Chen)
* Stephen Robinson, Microsoft Research [Understanding Inverse Document Frequency: On theoretical arguments for IDF](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.97.7340&rep=rep1&type=pdf)

### Bayesian Methodology

* Bayesian Programming languages: Software which allows you to specify the model without requiring you to specify the method:
  * [OpenBUGS](http://openbugs.net/w/FrontPage)
  * [JAGS](http://mcmc-jags.sourceforge.net/)
  * [STAN](http://mc-stan.org/)
* There is a super useful list of conjugate priors and interpretations on the [Conjugate Prior Wikipedia page](https://en.wikipedia.org/wiki/Conjugate_prior)!
* Monte Carlo:
  * Gamerman and Hedibert. [Markov chain Monte Carlo: stochastic simulation for Bayesian inference](http://www.dme.ufrj.br/mcmc/).
  * Doucet, Godsill, and Andrieu. "[On sequential Monte Carlo sampling methods for Bayesian filtering](https://www.cs.ubc.ca/~arnaud/doucet_godsill_andrieu_sequentialmontecarloforbayesfiltering.pdf)" Statistics and computing 10.3 (2000): 197-208.
  * Andrieu, Doucet, and Holenstein [Particle Markov chain Monte Carlo methods](https://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf)
* ABC:
  * Beaumont, Zhang, and Balding. "[Approximate Bayesian computation in population genetics](https://www.genetics.org/content/162/4/2025)." Genetics 162.4 (2002): 2025-2035.
  * Murray, Ghahramani, and MacKay. "[MCMC for doubly-intractable distributions](https://homepages.inf.ed.ac.uk/imurray2/pub/06doubly_intractable/doubly_intractable.pdf)" Proceedings of the 22nd Annual Conference on Uncertainty in Artificial Intelligence (UAI) (2006).
* Variational Inference:
  	* Blei, Kucukelbir and McAuliffe. "[ Variational Inference: A Review for Statisticians](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773)", JASA (2017): 859-877.
	* Blei and Jordan. "[Variational inference for Dirichlet process mixtures](https://projecteuclid.org/download/pdf_1/euclid.ba/1340371077)", Bayesian analysis 1.1 (2006): 121-143.
	* [A Beginner's Guide to Variational Methods](http://blog.evjang.com/2016/08/variational-bayes.html), by Eric Jang.


### Latent Dirichlet Allocation
* B. Barde and A. Bainwad. "[An overview of topic modeling methods and tools](https://ieeexplore.ieee.org/abstract/document/8250563)" (2017) ICICCS 745-750.
* Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "[Latent dirichlet allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)", Journal of machine Learning research 3.Jan (2003): 993-1022.
* Neural Networks approaches to document models:
  * [Mathgen](https://thatsmathematics.com/mathgen/) generates random papers
  * [Topic-RNN](https://github.com/dangitstam/topic-rnn) infers a topic model using a Neural Network

### Data science topic modelling
* [Preparing Data for Topic Modelling](https://publish.illinois.edu/commonsknowledge/2017/11/16/preparing-your-data-for-topic-modeling/)
* [NLP for legal documents](https://towardsdatascience.com/nlp-for-topic-modeling-summarization-of-legal-documents-8c89393b1534)
* [Machine-Learning-In-Law github repo](https://github.com/chibueze07/Machine-Learning-In-Law/tree/master)

### Judging topic models
* Chang, Jonathan, Jordan Boyd-Graber, Sean Gerrish, Chong Wang and David M. Blei. 2009. [Reading Tea Leaves: How Humans Interpret Topic Models](http://umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf). NIPS.
* Stevens, Kegelmeyer, Andrzejewsk and Buttler [Exploring Topic Coherence over many models and many topics](https://www.aclweb.org/anthology/D/D12/D12-1087.pdf)

### Data sources
* [Kaggle dataset for fake news](https://www.kaggle.com/nupursh/nlp-in-r-topic-modelling-for-fake-news)
* [Intelligence and Security Informatics Data Sets](https://www.azsecure-data.org/)
* [Vizsec security data collection](https://vizsec.org/data/)
* [Threatminer cyber data with NLP](https://www.threatminer.org/)
* [Phishing data corpus](https://monkey.org/~jose/phishing/phishing-2018)

### Worksheets (unassessed)

* [Worksheet 8.1 Topic Models and Bayes]({{ site.data.block08.ws01.url }}) 

## Navigation:

Previous: [Block 07](07.md).
Next: [Block 09](09.md).


