---
title: 6.0 Decision Trees and Random Forests
layout: coursebook
---
# 06 Decision Trees and Random Forests

In this block we cover:

* Decision Trees
  * The Classification and Regression Tree (CART) approach
  * Decision loss functions: ID3 vs Gini impurity
  * Pruning trees to reduce overfitting
  * Regression trees
* Random Forests
  * Ensembles of trees
  * Bagging features
  * Forests vs Boosted Decision Trees
  * Feature importance
  
## Lectures:

*  Decision Trees and Random Forests:
  * [{{ site.data.vid06.v0101.text }}]({{ site.data.vid06.v0101.url }})
    * [Slides]({{ site.data.block06.s0101.url }})
  * [{{ site.data.vid06.v0102.text }}]({{ site.data.vid06.v0102.url }})
    * [Slides]({{ site.data.block06.s0102.url }})
  * [Reference R code]({{ site.data.block06.c01.url }})


## Worksheets:

* [Worksheet 6.1 Trees, Forests, Decisions]({{ site.data.block06.ws01.url }}) 

## Workshop:

The workshop is split into two sections. The first of these is in R, and **generates the data** (so you should run it first). The second of these in in Python and compares to the R content. Note that the content is exported to the [DST github](https://github.com/dsbristol/dst/tree/master/data) and the code below grabs it from there, so it is possible to run it out of order.

* [{{ site.data.vid06.v0201.text }}]({{ site.data.vid06.v0201.url }})
  * [Rmd for 6.2 Workshop on Random Forests]({{ site.data.block06.s0201.url }})
* [{{ site.data.vid06.v0202.text }}]({{ site.data.vid06.v0202.url }})
  * [Python for 6.2 Workshop on Random Forests]({{ site.data.block06.s0202.url }})

## References

* Tree methods:
  * Chapter 9.2 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
  - [Penn State U Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) [How to prune trees](https://online.stat.psu.edu/stat508/lesson/11/11.8/11.8.2)
  - [Decision Tree Algorithms: Deep Math ML](https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1)
* Regression Trees:
  - Karalic A, ["Employing Linear Regression in Regression Tree Leaves"](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.3091) (1992) ECAI-92
* Boosted Decision Trees:
  - J. Elith, J. Leathwick, and T. Hastie ["A working guide to boosted regression trees"](https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2656.2008.01390.x) (2008). British Ecological Society.
* CART:
  -  CART = Classification and Regression Trees. Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984). Classification and regression trees.
  - [Wei-Yin Loh's 2011  Review](https://onlinelibrary.wiley.com/doi/full/10.1002/widm.8?casa_token=zVBiCZT6d24AAAAA%3AoZIV06S8oIjh5erpzsC0yzVmMFP6ilRntX9qfzmk8KNgKr-FKbWxCkxax1biS2eP8_o5h7bzPpkD5A) is popular.
  * ID3:  Quinlan, J. R. 1986. [Induction of Decision Trees.](https://link.springer.com/article/10.1007/BF00116251) Mach. Learn. 1, 1 (Mar. 1986), 81-106.
* Random Forests:
  * Chapter 15 of [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (Friedman, Hastie and Tibshirani).
  - [Implement a Random Forest From Scratch in Python](https://machinelearningmastery.com/implement-random-forest-scratch-python/)
  - [A Gentle Introduction to Random Forests at CitizenNet](http://blog.citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics)
  - [DataDive on Selecting good features](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)
  * [Cosma Shalizi on Regression Trees](http://www.stat.cmu.edu/~cshalizi/350-2006/lecture-10.pdf)
  * [Gilles Louppe PhD Thesis: Understanding Random Forests](https://arxiv.org/pdf/1407.7502.pdf)

Previous: [Block 05](05.md).
Next: [Block 07](07.md).
