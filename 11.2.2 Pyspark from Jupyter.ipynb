{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "administrative-bonus",
   "metadata": {},
   "source": [
    "# 11.2.2 Pyspark example\n",
    "\n",
    "This is a simple pyspark script that you can run locally or otherwise, provided that you have have a working Spark, pyspark and Jupyter configuration. See [Install notes](install_notes.txt) for details.\n",
    "\n",
    "## Generic setup of a Spark Context\n",
    "\n",
    "This is an essential component for every spark program. We are initialising it to allow for parallel computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"first app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-saturn",
   "metadata": {},
   "source": [
    "## Spark and output\n",
    "\n",
    "Spark assumes that you've done your homework and made sure that the output location is clear.\n",
    "\n",
    "This is often not the case when you are experimenting. You will therefore want to include provision for removing content that gets in the way.\n",
    "\n",
    "Note that you cannot do this easily from within Spark. It is best done from the OS or from python before you access the SparkContext.\n",
    "\n",
    "### A useful function to clear the output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "import os\n",
    "def clearaway(dir):\n",
    "    if os.path.exists(dir):\n",
    "        rmtree(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-layer",
   "metadata": {},
   "source": [
    "## 11.2.2.1 parallelize input and output\n",
    "\n",
    "\n",
    "This is a \"first example\" to show how input and output work in pyspark. Equivelent to `2.1-SparkInputOutput.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribute data over a spark context\n",
    "samples = sc.parallelize([\n",
    "    (\"abonsanto@fakemail.com\", \"Alberto\", \"Bonsanto\"),\n",
    "    (\"mbonsanto@fakemail.com\", \"Miguel\", \"Bonsanto\"),\n",
    "    (\"stranger@fakemail.com\", \"Stranger\", \"Weirdo\"),\n",
    "    (\"dbonsanto@fakemail.com\", \"Dakota\", \"Bonsanto\")\n",
    "])\n",
    "\n",
    "## Collect the data (a sequential operation)\n",
    "print(samples.collect())\n",
    "\n",
    "clearaway(\"output/folder\")\n",
    "## Save the data (a parallel operation)\n",
    "samples.saveAsTextFile(\"output/folder/here.txt\")\n",
    "## Load the data (a parallel operation)\n",
    "read_rdd = sc.textFile(\"output/folder/here.txt\")\n",
    "## Collect the data again\n",
    "print(read_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-edition",
   "metadata": {},
   "source": [
    "## 11.2.2.2 Exploiting RDD builtin functions\n",
    "\n",
    "RDDs have many useful functions. Count is one that is implemented efficiently in parallel.\n",
    "\n",
    "This is implemented in `2.2-SparkCount.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "## Count is parallelized\n",
    "counts = words.count()\n",
    "print(\"Number of elements in RDD -> %i\" % (counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-click",
   "metadata": {},
   "source": [
    "## 11.2.2.3 Accessing information about the Spark environment\n",
    "\n",
    "Pyspark has very many ways to access the details; `StorageLevel` is one, that describes the redundancy of the replication. You should investigate the other information available, e.g. [pyspark for beginners](https://www.tutorialspoint.com/pyspark/pyspark_storagelevel.htm).\n",
    "\n",
    "This is implemented in `2.3-SparkStorageLevel.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Parallelize an RDD\n",
    "rdd1 = sc.parallelize([1,2])\n",
    "## Set its storage level to \"2x replicated on memory and disk\"\n",
    "rdd1.persist(StorageLevel.MEMORY_AND_DISK_2 )\n",
    "## Get this report\n",
    "rdd1.getStorageLevel()\n",
    "print(rdd1.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-syntax",
   "metadata": {},
   "source": [
    "## 11.2.2.4 A simple filter\n",
    "\n",
    "Filtering is the best thing to do with distributed data as it can return a manageable amount of data. This is trivially achieved via a `filter`, as in the following.\n",
    "\n",
    "This is implemented in `2.4-ReadAndFilter.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile = \"data/books/5720.txt\"  \n",
    "logData = sc.textFile(logFile).cache()\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "print (\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-afghanistan",
   "metadata": {},
   "source": [
    "## 11.2.2.5 Machine Learning from within PySpark\n",
    "\n",
    "For data science, the real joy of spark is distributed machine learning.\n",
    "\n",
    "The main library is called `mllib` and it contains many useful functions; see e.g. [Mllib for beginners](https://www.tutorialspoint.com/pyspark/pyspark_mllib.htm).\n",
    "\n",
    "This is implemented in `2.5-MLlibRecommender.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "clearaway(\"output/tmp/myCollaborativeFilter\") \n",
    "\n",
    "## Here we have to be careful to protect workers so that they receive instruction from the main thread.\n",
    "if __name__ == \"__main__\":\n",
    "   data = sc.textFile(\"data/test_data.csv\")\n",
    "    ## Map data from a string into a Rating of person i of data j\n",
    "   ratings = data.map(lambda l: l.split(','))\\\n",
    "      .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "   \n",
    "   # Build the recommendation model using Alternating Least Squares\n",
    "   rank = 10\n",
    "   numIterations = 10\n",
    "   model = ALS.train(ratings, rank, numIterations)\n",
    "   \n",
    "   # Evaluate the model on training data\n",
    "   testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "   predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "   ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "   MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "   print(\"Mean Squared Error = \" + str(MSE))\n",
    "   \n",
    "   # Save and load model\n",
    "   model.save(sc, \"output/tmp/myCollaborativeFilter\")\n",
    "   sameModel = MatrixFactorizationModel.load(sc, \"output/tmp/myCollaborativeFilter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-demand",
   "metadata": {},
   "source": [
    "## 11.2.2.6 MapReduce Example to count word frequencies\n",
    "\n",
    "The wonderful thing about Spark is that it will parallelise serial content, and retain any parallelisation available in sequential content. So if we provide it with data stored on HDFS, it stays that way, with no additional effort.\n",
    "\n",
    "We can create a parallelised interface from a regular file structure just by reading many files at once.\n",
    "\n",
    "Here we use Map Reduce from PySpark taking advantage of this structure.\n",
    "\n",
    "This is implemented in `2.6-MapReduceWordcount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "clearaway(\"pyspark_wc\") \n",
    "\n",
    "#text_file = sc.textFile(\"hdfs/books/*.txt\")\n",
    "text_file = sc.textFile(\"data/books/*.txt\")\n",
    "def linesplit(line):\n",
    "    line = re.sub(r'[^\\w\\s]','',line)\n",
    "    return(line.split(\" \"))\n",
    "\n",
    "counts = text_file.flatMap(linesplit) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "counts.saveAsTextFile(\"output/pyspark_wc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-circular",
   "metadata": {},
   "source": [
    "## 11.2.2.7 A working machine learning pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## See https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa\n",
    "\n",
    "## Interactive session\n",
    "\n",
    "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
    "df = spark.read.csv('data/bank.csv', header = True, inferSchema = True)\n",
    "df.printSchema()\n",
    "\n",
    "## Converting to pandas\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the spark RDS describe\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
    "df.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Two ways to sample 1000 random points\n",
    "## The recommended way, though you have to specify two limits (the fraction, here 0.5, and the number, here 1000, if needed)\n",
    "## This way creates a spark RDD\n",
    "numeric_data1=df.sample(False, 0.5, seed=0).limit(1000)\n",
    "numeric_data = numeric_data1.toPandas()\n",
    "numeric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This way does a \"collect\", which can be slow in larger datasets, but is more natural\n",
    "numeric_data2=df.rdd.takeSample(False, 1000, seed=0)\n",
    "numeric_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-norway",
   "metadata": {},
   "source": [
    "Scatter matrices are a good go-to plot to understand the structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=pd.plotting.scatter_matrix(numeric_data, diagonal='kde', marker='+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## Preparing the data\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], \n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting rid of useless columns\n",
    "df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\n",
    "cols = df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "## Making a \"data processing pipeline\"\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "## Look at a couple of rows\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "## Do a standard training/ testing split\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Fit a logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Plot the results\n",
    "import numpy as np\n",
    "\n",
    "beta = np.sort(lrModel.coefficients)\n",
    "\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.savefig('output/bank_betas.png')\n",
    "#plt.close()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################\n",
    "## ROC curves\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "predpandas=predictions.select(['label','probability']).toPandas()\n",
    "predpandas['probability']=[x[1] for x in predpandas['probability']]\n",
    "from sklearn.metrics import roc_curve\n",
    "testrocarray = roc_curve(predpandas['label'], predpandas['probability'])\n",
    "testroc = pd.DataFrame.from_records(testrocarray).transpose()\n",
    "testroc.columns=['FPR','TPR','Thresh']\n",
    "\n",
    "trainroc = trainingSummary.roc.toPandas()\n",
    "\n",
    "plt.plot(trainroc['FPR'],trainroc['TPR'],label='Training')\n",
    "plt.plot(testroc['FPR'],testroc['TPR'],label='Test')\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.savefig('output/banks_roc.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-outreach",
   "metadata": {},
   "source": [
    "## 11.2.2.8 Broadcast\n",
    "\n",
    "You might want to update all worker nodes with new information, e.g. hyperparameter values, etc. To do this you can use a **Broadcast** which simply assigns the variable to all worker nodes.\n",
    "\n",
    "This is implemented in `Supplement/pys_broadcast.py` as it is so trivially explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"])\n",
    "data = words_new.value\n",
    "print (\"Stored data -> %s\" % (data) )\n",
    "elem = words_new.value[2]\n",
    "print (\"Printing a particular element in RDD -> %s\" % (elem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-provision",
   "metadata": {},
   "source": [
    "## Supplementary materials\n",
    "\n",
    "For completeness, additional material are placed in `Supplement` and include:\n",
    "    \n",
    "* pys_filter.py: A simple example of filtering\n",
    "* pys_map.py: A simple example of Map\n",
    "* pys_collect.py: A simple example of a Collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-requirement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
